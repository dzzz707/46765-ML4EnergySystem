{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 \n",
    "## Compute Target values\n",
    "\n",
    "The historic price and wind production data is used to determine what would have been the optimal bidding strategy in the energy market. The resuting passed optimal bids are used to build model 2. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Helping Method to limit the  \n",
    "def ObservationPeriod(Start_Observation, End_Observation, data):\n",
    "    Start_Observation = pd.to_datetime(Start_Observation)\n",
    "    End_Observation = pd.to_datetime(End_Observation)\n",
    "    condition = (data[\"HourDK\"]>= Start_Observation) & (data[\"HourDK\"]< End_Observation)\n",
    "    data  = data[condition]\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# p_t_hat as decision variable\n",
    "def OptimizationProblemEnergybid(data):\n",
    "    model = gp.Model()\n",
    "    model.setParam('OutputFlag', 0)\n",
    "    solutions  = list()\n",
    "    capacity = 6000.0\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        \n",
    "        if pd.isnull(row[['p_t', 'DA_PriceEUR', 'BalancingPriceDownEUR', 'BalancingPriceUpEUR']]).any():\n",
    "            continue\n",
    "        \n",
    "        p_t_hat = model.addVar(name=\"p_t_hat\", lb=0, ub=capacity, vtype=GRB.INTEGER)\n",
    "        z_down = model.addVar(name=\"z_down\", vtype=GRB.CONTINUOUS, lb=0)\n",
    "        z_up = model.addVar(name=\"z_up\", vtype=GRB.CONTINUOUS, lb=0)\n",
    "        \n",
    "        condition_down = model.addVar(name=\"condition_down\", vtype=GRB.BINARY)\n",
    "        condition_up = model.addVar(name=\"condition_up\", vtype=GRB.BINARY)\n",
    "        \n",
    "        model.addConstr(z_up <= (p_t_hat - row['p_t']) * condition_up)\n",
    "        model.addConstr(z_up >= (p_t_hat - row['p_t']) * condition_up)\n",
    "        model.addConstr(z_down <= (row['p_t'] - p_t_hat) * condition_down)\n",
    "        model.addConstr(z_down >= (row['p_t'] - p_t_hat) * condition_down)\n",
    "        model.addConstr(condition_up + condition_down == 1)\n",
    "        \n",
    "    \n",
    "        #defining the constraints\n",
    "        model.addConstr(-p_t_hat <= 0)\n",
    "        model.addConstr(p_t_hat <= capacity)\n",
    "        model.addConstr(-z_down <= 0)\n",
    "        model.addConstr(-z_up <= 0)\n",
    "        \n",
    "        model.setObjective((row['DA_PriceEUR']*p_t_hat)+(row['BalancingPriceDownEUR']*z_down-row['BalancingPriceUpEUR']*z_up), GRB.MAXIMIZE)\n",
    "        \n",
    "        model.optimize()\n",
    "    \n",
    "        if model.status == GRB.OPTIMAL:\n",
    "            \n",
    "            values = {\"Day-Ahead-Price\": row['DA_PriceEUR'],\n",
    "                      \"BalancingPriceUp\":row['BalancingPriceUpEUR'],\n",
    "                      \"BalancingPriceDown\":row['BalancingPriceDownEUR'],\n",
    "                      \"p_t\":row['p_t'],\n",
    "                      \"Revenue\":model.ObjVal\n",
    "                      }\n",
    "            values.update({v.varName: v.x for v in model.getVars()})\n",
    "        \n",
    "            \n",
    "            solutions.append({row['HourDK']:values})\n",
    "                \n",
    "        elif model.status == GRB.INFEASIBLE:\n",
    "            print(\"Model is infeasible.\")\n",
    "            model.computeIIS()\n",
    "            model.write(\"infeasible.ilp\")  # Write IIS to a file for review\n",
    "            for c in model.getConstrs():\n",
    "                if c.IISConstr:\n",
    "                    print(f\"Infeasible constraint: {c.constrName}\")\n",
    "            \n",
    "            \n",
    "        elif model.status == GRB.UNBOUNDED:\n",
    "            print(\"Model is unbounded.\")\n",
    "        else:\n",
    "            print(\"Model status:\", model.status)\n",
    "            \n",
    "        model.remove(model.getConstrs())\n",
    "        model.remove(model.getVars())\n",
    "        \n",
    "    solutions = pd.DataFrame([v for d in solutions for v in d.values()],\n",
    "                  index=[ts for d in solutions for ts in d.keys()])    \n",
    "    \n",
    "    return solutions"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prices_df = pd.read_csv('./Data assignment 1/prices_merged_df_output.csv', delimiter=',')\n",
    "features_df = pd.read_csv('./Data assignment 1/Feature data.csv', delimiter=',')\n",
    "prices_df = pd.merge(prices_df, features_df, how='inner', left_on='HourDK', right_on='datetime')\n",
    "\n",
    "prices_df['AKI Kalby Active Power'] = (-1 * prices_df['AKI Kalby Active Power'])\n",
    "data = prices_df.copy()\n",
    "data.rename(columns={'AKI Kalby Active Power': 'p_t'}, inplace=True)\n",
    "data[\"HourDK\"] = pd.to_datetime(data[\"HourDK\"])\n",
    "\n",
    "data = ObservationPeriod(\"2022-01-01 00:00:00\", \"2022-08-31 23:00:00\", data)\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "prices_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "solutions = OptimizationProblemEnergybid(data)\n",
    "\n",
    "df_target = pd.DataFrame({\n",
    "    'p_t_hat': solutions['p_t_hat']\n",
    "})\n",
    "\n",
    "data.rename(columns={'p_t': 'actProd'}, inplace=True)\n",
    "\n",
    "df_target.to_csv(\"./Data assignment 1/TargetValues_Model2.csv\", index=True)\n",
    "data.to_csv(\"./Data assignment 1/Features_Model2.csv\", index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the required data\n",
    "target_filepath = 'Data assignment 1/TargetValues_Model2.csv'\n",
    "prices_df = pd.read_csv('./Data assignment 1/prices_merged_df_output.csv', delimiter=',')\n",
    "target_values_df = pd.read_csv(target_filepath)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# p_t_hat is the optimal power production that should be offered. \n",
    "target_values_df.rename(columns={'Unnamed: 0': 'DateTime'}, inplace=True)\n",
    "target_values_df.set_index('DateTime', inplace=True)\n",
    "target_values_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check the descriptive statistics of the dataset\n",
    "target_values_df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptive statistics indicate that 25% of the data values are 0.0. This means that 25% of the time, at least, the offer should be 0. The 50th percentile being 1366 indicates that 25% of the data lies between 0 and 1366. The 75th percentile (or the third quartile, Q3) is 6000. This means that 75% of the data values are less than or equal to 30000. This indicates that the upper range of values is much higher than the median, pointing to the fact that thereâ€™s a significant jump in values for the highest 25% of the data. The 25th percentile being zero indicates that a significant portion of the data is centered around zero which leads us to the assumption that this would be a good cluster. Similarly, the 75th percentile being equal to the maximum (6000) implies that the top 25% of the data values are all 6000. This indicates that 6000 is another significant grouping and likely forms a separate cluster.\n",
    "\n",
    "Given this, we can reasonably conclude that the dataset likely has three clusters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set a range for the possible number of clusters\n",
    "ks = range(1, 6)\n",
    "# Create an empty list to save the distortions\n",
    "distortions=[]\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k,n_init='auto').fit(target_values_df)\n",
    "    model.fit(target_values_df)\n",
    "    distortions.append(sum(np.min(cdist(target_values_df, model.cluster_centers_,'euclidean'), axis=1)) / target_values_df.shape[0])\n",
    "\n",
    "# Visualize the Elbow method graph    \n",
    "plt.plot(ks, distortions, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method using Distortion')\n",
    "plt.show()\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cluster the data based on the number of clusters from the elbow method\n",
    "kmeans=KMeans(n_clusters=3,n_init='auto')\n",
    "model=kmeans.fit(target_values_df)\n",
    "target_values_df['Cluster']=kmeans.labels_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The average value of each cluster\n",
    "cluster_summary = target_values_df.groupby('Cluster').mean().round()\n",
    "print(cluster_summary)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Against expectation, there is no zero-cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize the clusters\n",
    "plt.scatter(target_values_df.index, target_values_df['p_t_hat'], c=target_values_df['Cluster'], s=50, edgecolors='black', cmap ='inferno')\n",
    "plt.xlabel('Datetime', )\n",
    "plt.ylabel('Optimal Energy Bid (MW)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(6))  # Show only 10 date labels\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "plt.title('K-Means Clustering of Energy Bids')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(kmeans.cluster_centers_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "features = pd.read_csv('Data assignment 1/Features_Model2.csv')\n",
    "features = features[['HourDK', 'BalancingPriceUpEUR', 'BalancingPriceDownEUR', 'DA_PriceEUR', 'actProd']]\n",
    "\n",
    "features.rename(columns={'HourDK': 'DateTime'}, inplace=True)\n",
    "features.set_index('DateTime', inplace=True)\n",
    "scaler = StandardScaler()\n",
    "features['actProd'] = scaler.fit_transform(features[['actProd']])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "features",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "target = pd.read_csv('Data assignment 1/TargetValues_Model2.csv')\n",
    "target.rename(columns={'Unnamed: 0': 'DateTime'}, inplace=True)\n",
    "target.set_index('DateTime', inplace=True)\n",
    "scaler = StandardScaler()\n",
    "target['p_t_hat'] = scaler.fit_transform(target[['p_t_hat']])\n",
    "target"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "We will use regression to predict the target value since it can be seen that the target values can not be divided into distinct classes very well. Linear regression is most likely not complex enough, and it was shown that the least squares model for nonlinear regression showed the same results as linear regression. This is why the choice is now made to use polynomial nonlinear regression for model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First determine the best degree used for polynomial nonlinear regression. The same code is used as for model 1."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define cross-validation function, which returns RMSE\n",
    "def perform_cross_validation(X, y, degree, n_splits=10):\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    # Initialize linear regression model\n",
    "    linear_model = LinearRegression()\n",
    "    \n",
    "    # Use KFold for cross-validation, n_splits set to 10\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Compute RMSE via cross-validation (use negative MSE and then take the square root)\n",
    "    neg_mse_scores = cross_val_score(linear_model, X_poly, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # Convert negative MSE to RMSE\n",
    "    rmse_scores = np.sqrt(-neg_mse_scores)\n",
    "    \n",
    "    # Return the mean RMSE\n",
    "    return rmse_scores.mean()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Polynomial degrees to evaluate: 2, 3, 4 and 5\n",
    "degrees = [2, 3, 4, 5]\n",
    "best_degree = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "# Perform cross-validation for each polynomial degree\n",
    "for degree in degrees:\n",
    "    print(f\"Evaluating degree {degree} polynomial: \")\n",
    "    rmse = perform_cross_validation(features, target, degree)\n",
    "    print(f\"Mean RMSE for degree {degree}: {rmse:.5f}\")\n",
    "    \n",
    "    # Update the best model based on RMSE\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_degree = degree"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "features",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"Best degree is {best_degree} with RMSE: {best_rmse:.5f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate the best polynomial features\n",
    "poly_best = PolynomialFeatures(degree=best_degree, include_bias=False)\n",
    "X_poly_best = poly_best.fit_transform(features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_poly_best, X_test_poly_best, y_train_poly_best, y_test_poly_best = train_test_split(\n",
    "    X_poly_best, target.squeeze(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the final polynomial model\n",
    "nonlinear_model_best = LinearRegression()\n",
    "nonlinear_model_best.fit(X_train_poly_best, y_train_poly_best)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_poly_best_test = nonlinear_model_best.predict(X_test_poly_best)\n",
    "\n",
    "test_rmse_best = np.sqrt(mean_squared_error(y_test_poly_best, y_pred_poly_best_test))\n",
    "\n",
    "# Evaluation\n",
    "print(f\"RMSE for Model 2 with degree {best_degree}: {test_rmse_best:.5f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Revenue calculation\n",
    "In order to properly compare the revenue with model 1, the revenue is also calculated from September 1st until December 31st. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prices_df = pd.read_csv('./Data assignment 1/prices_merged_df_output.csv', delimiter=',')\n",
    "features_df = pd.read_csv('./Data assignment 1/Feature data.csv', delimiter=',')\n",
    "prices_df = pd.merge(prices_df, features_df, how='inner', left_on='HourDK', right_on='datetime')\n",
    "\n",
    "prices_df['AKI Kalby Active Power'] = (-1 * prices_df['AKI Kalby Active Power'])\n",
    "data = prices_df.copy()\n",
    "data.rename(columns={'AKI Kalby Active Power': 'p_t'}, inplace=True)\n",
    "data[\"HourDK\"] = pd.to_datetime(data[\"HourDK\"])\n",
    "\n",
    "data_pred = ObservationPeriod(\"2022-09-01 00:00:00\", \"2022-12-31 23:00:00\", data)\n",
    "\n",
    "data_pred.reset_index(drop=True, inplace=True)\n",
    "prices_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "solutions_pred = OptimizationProblemEnergybid(data_pred)\n",
    "\n",
    "df_target_pred = pd.DataFrame({\n",
    "    'p_t_hat': solutions['p_t_hat']\n",
    "})\n",
    "# data_pred.rename(columns={'p_t': 'actProd'}, inplace=True)\n",
    "# \n",
    "# df_target.to_csv(\"./Data assignment 1/TargetValues_Model2.csv\", index=True)\n",
    "# data.to_csv(\"./Data assignment 1/Features_Model n92.csv\", index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "solutions_pred",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datapred=data[['HourDK', 'BalancingPriceUpEUR', 'BalancingPriceDownEUR', 'DA_PriceEUR', 'actProd']]\n",
    "datapred.rename(columns={'HourDK': 'DateTime'}, inplace=True)\n",
    "datapred.set_index('DateTime', inplace=True)\n",
    "scaler = StandardScaler()\n",
    "datapred['actProd'] = scaler.fit_transform(datapred[['actProd']])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_poly_best_pred = poly_best.fit_transform(data_pred)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "hat_p = nonlinear_model_best.predict(X_poly_best)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lambda_day_ahead = datapred['DA_PriceEUR'].values  \n",
    "lambda_up = datapred['BalancingPriceUpEUR'].values\n",
    "lambda_down = datapred['BalancingPriceDownEUR'].values\n",
    "p=datapred['actProd'].values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "c = 6000\n",
    "z_down = np.maximum(p - hat_p, 0)\n",
    "z_up = np.maximum(hat_p - p, 0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "revenue = np.sum(lambda_day_ahead * hat_p) + np.sum(lambda_down * z_down - lambda_up * z_up)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "revenue*-1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noah",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
